---
title: "分類（性能指標）"
output: html_document
---

```{r, message=FALSE}
library(tidyverse)
library(caret)
```

## 分類の性能指標

### 正解率

**ここで示すコードは説明のためのものであり，分類問題を解くために必要なものではない。（つまり，書けなくてもよい。）**

個体（全10件）が0（陰性）と1（陽性）のどちらに属するかを診断するアルゴリズムが，次のような結果を出したとする。
（補足：出力変数は数値ではなくfactorである。）

```{r}
my_data <- data.frame(
  answer = factor(c(0,   1,   1,   0,   1,   0,   1,    0,   0,   1)),  # 正解
  prob =          c(0.7, 0.8, 0.3, 0.4, 0.9, 0.6, 0.99, 0.1, 0.2, 0.5)) # 陽性確率
```

陽性確率の大きい順に並び替えておく。

```{r}
(my_data <- my_data %>% arrange(desc(prob)))
```

（仮に）上位6件が陽性（1）だとする。

```{r}
n <- nrow(my_data) # データの件数（10）
my_data$prediction <- factor(c(1, 1, 1, 1, 1, 1, 0, 0, 0, 0))
# my_data$prediction <- factor(c(rep(1, 6), rep(0, n - 6))) # この方が汎用的
```

以下を数える。

* 陽性（positive）：（例：病気だと診断された。）
* 真陽性（true positive）：（例：病気だと診断され，実際に病気だった。）
* 偽陽性（false positive）：（例：病気だと診断されたが，実際は病気ではなかった。）「第1種の過誤」とも呼ばれる。
* 陰性（negative）：（例：病気ではないと診断された。）
* 真陰性（true negative）：（例：病気ではないと診断され，実際に病気でなかった。）
* 偽陰性（false negative）：（例：病気ではないと診断されたが，実際は病気だった。）「第2種の過誤」とも呼ばれる。

まとめて集計する方法を後で紹介するが，まずは一つずつ数えてみる。（見やすくするために，小文字で`my_`を付けるという，名前の原則を無視する。）

```{r}
(P <- sum(my_data$prediction == 1)) # 陽性
```

```{r}
(TP <- sum(my_data$prediction == 1 & my_data$answer == 1)) # 真陽性
```

```{r}
(FP <- sum(my_data$prediction == 1 & my_data$answer == 0)) # 偽陽性
```

```{r}
(N <- sum(my_data$prediction == 0)) # 陰性
```

```{r}
(TN <- sum(my_data$prediction == 0 & my_data$answer == 0)) # 真陰性
```

```{r}
(FN <- sum(my_data$prediction == 0 & my_data$answer == 1)) # 偽陰性
```

まとめて集計するなら次のとおり。このように，真陽性，偽陽性，真陰性，偽陰性の数を数えてまとめたものを**混同行列**（confusion matrix）と呼ぶ。

```{r}
table(my_data$prediction, my_data$answer)
```

先に紹介した**正解率（accuracy）**は，真陽性（TP），偽陽性（FP），真陰性（TN），偽陰性（FN）を使って次のように求められる。

```{r}
(ACC <- (TP + TN) / (TP + FP + TN + FN))
```

### 精度と再現率

陽性に対する真陽性の割合を**精度（precision）**と呼ぶ。適合率，信頼度と呼ばれることもある。

```{r}
(my_precision <- TP / (TP + FP))
```

（真陽性＋偽陰性）（つまりmy_answerが1の件数）に対する真陽性の割合を**再現率（recall）**，**真陽性率（true positive rate）**と呼ぶ。後で紹介する感度も同じもの。

```{r}
(my_recall <- TP / (TP + FN))
```

精度と再現率の調和平均を**F値**（F-measure）と呼ぶ。F1スコア（F1 score）やFスコア（F-score）と呼ばれることもある。

```{r}
1 / ((1/my_precision + 1/my_recall)/2)
```

補足：精度と再現率は情報検索の分野でよく使われる指標である。

### 感度と特異度

先に紹介した再現率は，**感度（sensitivity）**とも呼ばれる。

```{r}
my_sensitivity <- TP / (TP + FN)
```

（真陰性＋偽陽性）（つまりmy_answerが0の件数）に対する真陰性の割合を**特異度（specificity）**と呼ぶ。真陰性率（true negative rate）と呼ばれることもある。

```{r}
my_specificity <- TN / (TN + FP)
```

補足：感度や特異度は疫学の分野でよく使われる用語である。

### その他の指標

**k統計量**（kappa statistic）を，予想が偶然当たる確率つまり，陽性と予測する確率×疾患ありの確率 + 陰性と予測する確率×疾患なしの確率をeとして，次のように定義する。これは，正解率からランダムに予測しても当たる可能性を差し引いたものである。

```{r}
e <- (P / n) * (TP + FN) / n + (N / n) * (TN + FP) / n
(ACC - e)/(1 - e)
```

分類に関してよく使われる性能指標には次のようなものがある。

指標|定義|本文での値|補足
--|--|--|--
accuracy（正解率）|(TP + TN) / (TP + FP + TN + FN)|`r sprintf('%.3f', (TP + TN) / (TP + FP + TN + FN))`|正答率とも呼ばれる。
Kappa（k統計量）|(ACC - e) / (1 - e)|`r sprintf('%.3f', (ACC - e)/(1 - e))`|eは(P/n)*(TP+FN)/n+(N/n)*(TN+FP)/n
sensitivity（感度）|TP / (TP + FN)|`r sprintf('%.3f', TP / (TP + FN))`|再現率，真陽性率と同じ。
specificity（特異度）|TN / (TN + FP)|`r sprintf('%.3f', TN / (TN + FP))`|真陰性率と同じ。
positive predictive value（陽性適中度）|TP / (TP + FP)|`r sprintf('%.3f', TP / (TP + FP))`|精度と同じ。適合率とも呼ばれる。
negative predictive value（陰性的中度）|TN / (TN + FN)|`r sprintf('%.3f', TN / (TN + FN))`|
precision（精度）|TP / (TP + FP)|`r sprintf('%.3f', TP / (TP + FP))`|陽性適中度と同じ。
recall（再現率）|TP / (TP + FN)|`r sprintf('%.3f', TP / (TP + FN))`|感度，真陽性率と同じ。
F値|精度と再現率の調和平均|`r sprintf('%.3f', 1 / ((1/my_precision + 1/my_recall)/2))`|
prevalence（有病率）|(TP + FN) / (TP + FP + TN + FN)|`r sprintf('%.3f', (TP + FN) / (TP + FP + TN + FN))`|
detection rate（検出率）|TP / (TP + FP + TN + FN)|`r sprintf('%.3f', TP / (TP + FP + TN + FN))`|
detection prevalence|(TP + FP) / (TP + FP + TN + FN)|`r sprintf('%.3f', (TP + FP) / (TP + FP + TN + FN))`|
balanced accuracy|(感度+特異度) / 2|`r sprintf('%.3f', (my_sensitivity + my_specificity)/2)`|
true positive rate（真陽性率）|TP / (TP + FN)|`r sprintf('%.3f', TP / (TP + FN))`|感度，再現率と同じ。検出力とも呼ばれる。
false positive rate（偽陽性率）|FP / (TN + FP)|`r sprintf('%.3f', FP / (TN + FP))`|（1-特異度）である。

`confusionMatrix()`を使って，これらの指標をまとめて求める。（`positive = "1"`のように，何が陽性なのかを指定するのが大切で，これを忘れると，陽性と陰性を逆にして計算されてしまうことがある。）

```{r}
confusionMatrix(data = my_data$prediction, reference = my_data$answer, positive = "1", mode = "everything")
```

正解率とk統計量は，二値分類でない場合にも使える。

**練習：** 確率の大きいものからm人を陽性と見なした場合の，true positive rateとfalse positive rateを，mを0から10まで変化させて調べよ。これは性能指標に慣れるために手作業で行うことを想定した練習だが，自動化したければ次のようにすればよい．

```{r}
f <- function(m) {
  my_data$prediction  <- factor(c(rep(1, m), rep(0, n - m)))
  TP <- sum(my_data$prediction == 1 & my_data$answer == 1)
  FP <- sum(my_data$prediction == 1 & my_data$answer == 0)
  TN <- sum(my_data$prediction == 0 & my_data$answer == 0)
  FN <- sum(my_data$prediction == 0 & my_data$answer == 1)
  FPR <- FP / (TN + FP)
  TPR <- TP / (TP + FN)
  list(fpr = FPR, tpr = TPR)
}

my_result <- map_dfr(0:10, f)

ggplot(data = my_result, mapping = aes(x = fpr, y = tpr)) +
  geom_point() +
  geom_line()
# plot(my_result, type = "o") # でもよい
```

### ROC曲線

```{r, eval=FALSE}
install.packages('ROCR')
```

```{r}
# このコードは理解できなくてよい。
library(ROCR)

my_pred <- prediction(predictions = my_data$prob, labels = my_data$answer)
my_perf <- performance(prediction.obj = my_pred, measure = "tpr", x.measure = "fpr")
plot(my_perf)
```

ROC曲線の下の部分の面積をAUC（area under the curve）と呼ぶ。この値が1に近いほど分類器の性能がよいことが期待される。（ランダムな分類器のAUCは0.5）

```{r}
performance(prediction.obj = my_pred, measure = "auc")@y.values[[1]]
```

### 練習

タイタニック（その1）の学習におけるROC曲線を描いてみよう。

```{r,cache=TRUE}
my_data <- epitools::expand.table(Titanic)
my_index <- sample(1:nrow(my_data), 0.8 * nrow(my_data))
my_train <- my_data[my_index, ]
my_test <- my_data[-my_index, ]
my_result <- train(form = Survived ~ ., data = my_train, method = "knn")
my_train_prediction <- predict(object = my_result, newdata = my_train)
confusionMatrix(data = my_train_prediction, reference = my_train$Survived, positive = "Yes", mode = "everything")
```

ROC曲線を描くためには，陽性となる確率が必要。そのための引数`type = "prob"`を`predict()`に追加する．

```{r}
tmp <- predict(object = my_result, newdata = my_train, type = "prob")
head(tmp)
```

Yesの列が陽性となる確率で，`my_prob$Yes`で取り出せる。

```{r}
# このコードは理解できなくてよい。
library(ROCR)

my_prob <- tmp$Yes
my_pred <-prediction(predictions = my_prob, labels = my_train$Survived)
my_perf <- performance(prediction.obj = my_pred, measure = "tpr", x.measure = "fpr")
plot(my_perf)
```

AUCを求める。

```{r}
performance(prediction.obj = my_pred, measure = "auc")@y.values[[1]]
```

**レポート課題10：タイタニック（その1）のデータを使った教師あり学習において，4つ以上の手法で，AUCと正解率を求め，比較してください。**